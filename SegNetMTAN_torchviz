digraph {
	graph [size="202.79999999999998,202.79999999999998"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2462638835168 [label="
 (2, 13, 288, 384)" fillcolor=darkolivegreen1]
	2462533884112 [label=LogSoftmaxBackward0]
	2462533885840 -> 2462533884112
	2462533885840 [label=ConvolutionBackward0]
	2462638526800 -> 2462533885840
	2462638526800 [label=ConvolutionBackward0]
	2462638526944 -> 2462638526800
	2462638526944 [label=MulBackward0]
	2462638527136 -> 2462638526944
	2462638527136 [label=SigmoidBackward0]
	2462638527280 -> 2462638527136
	2462638527280 [label=CudnnBatchNormBackward0]
	2462638527376 -> 2462638527280
	2462638527376 [label=ConvolutionBackward0]
	2462638527568 -> 2462638527376
	2462638527568 [label=ReluBackward0]
	2462638527760 -> 2462638527568
	2462638527760 [label=CudnnBatchNormBackward0]
	2462638527856 -> 2462638527760
	2462638527856 [label=ConvolutionBackward0]
	2462638528048 -> 2462638527856
	2462638528048 [label=CatBackward0]
	2462638528240 -> 2462638528048
	2462638528240 [label=MaxUnpool2DBackward0]
	2462638528384 -> 2462638528240
	2462638528384 [label=ReluBackward0]
	2462638528480 -> 2462638528384
	2462638528480 [label=CudnnBatchNormBackward0]
	2462638528576 -> 2462638528480
	2462638528576 [label=ConvolutionBackward0]
	2462638528768 -> 2462638528576
	2462638528768 [label=ReluBackward0]
	2462638528960 -> 2462638528768
	2462638528960 [label=CudnnBatchNormBackward0]
	2462638529056 -> 2462638528960
	2462638529056 [label=ConvolutionBackward0]
	2462638529248 -> 2462638529056
	2462638529248 [label=MaxUnpool2DBackward0]
	2462638529440 -> 2462638529248
	2462638529440 [label=ReluBackward0]
	2462638529536 -> 2462638529440
	2462638529536 [label=CudnnBatchNormBackward0]
	2462638529632 -> 2462638529536
	2462638529632 [label=ConvolutionBackward0]
	2462638529824 -> 2462638529632
	2462638529824 [label=ReluBackward0]
	2462638530016 -> 2462638529824
	2462638530016 [label=CudnnBatchNormBackward0]
	2462638530112 -> 2462638530016
	2462638530112 [label=ConvolutionBackward0]
	2462638530304 -> 2462638530112
	2462638530304 [label=ReluBackward0]
	2462638530496 -> 2462638530304
	2462638530496 [label=CudnnBatchNormBackward0]
	2462638530592 -> 2462638530496
	2462638530592 [label=ConvolutionBackward0]
	2462638530784 -> 2462638530592
	2462638530784 [label=MaxUnpool2DBackward0]
	2462638530976 -> 2462638530784
	2462638530976 [label=ReluBackward0]
	2462638531072 -> 2462638530976
	2462638531072 [label=CudnnBatchNormBackward0]
	2462638531168 -> 2462638531072
	2462638531168 [label=ConvolutionBackward0]
	2462638531360 -> 2462638531168
	2462638531360 [label=ReluBackward0]
	2462638531552 -> 2462638531360
	2462638531552 [label=CudnnBatchNormBackward0]
	2462638531648 -> 2462638531552
	2462638531648 [label=ConvolutionBackward0]
	2462638531840 -> 2462638531648
	2462638531840 [label=ReluBackward0]
	2462638532032 -> 2462638531840
	2462638532032 [label=CudnnBatchNormBackward0]
	2462638532128 -> 2462638532032
	2462638532128 [label=ConvolutionBackward0]
	2462638532320 -> 2462638532128
	2462638532320 [label=MaxUnpool2DBackward0]
	2462638532512 -> 2462638532320
	2462638532512 [label=ReluBackward0]
	2462638532608 -> 2462638532512
	2462638532608 [label=CudnnBatchNormBackward0]
	2462638532704 -> 2462638532608
	2462638532704 [label=ConvolutionBackward0]
	2462638532896 -> 2462638532704
	2462638532896 [label=ReluBackward0]
	2462638533088 -> 2462638532896
	2462638533088 [label=CudnnBatchNormBackward0]
	2462638533184 -> 2462638533088
	2462638533184 [label=ConvolutionBackward0]
	2462638533376 -> 2462638533184
	2462638533376 [label=ReluBackward0]
	2462638533568 -> 2462638533376
	2462638533568 [label=CudnnBatchNormBackward0]
	2462638533664 -> 2462638533568
	2462638533664 [label=ConvolutionBackward0]
	2462638533856 -> 2462638533664
	2462638533856 [label=MaxUnpool2DBackward0]
	2462638534048 -> 2462638533856
	2462638534048 [label=MaxPool2DWithIndicesBackward0]
	2462638534144 -> 2462638534048
	2462638534144 [label=ReluBackward0]
	2462638534240 -> 2462638534144
	2462638534240 [label=CudnnBatchNormBackward0]
	2462638534336 -> 2462638534240
	2462638534336 [label=ConvolutionBackward0]
	2462638534528 -> 2462638534336
	2462638534528 [label=ReluBackward0]
	2462638534720 -> 2462638534528
	2462638534720 [label=CudnnBatchNormBackward0]
	2462638534816 -> 2462638534720
	2462638534816 [label=ConvolutionBackward0]
	2462638535008 -> 2462638534816
	2462638535008 [label=ReluBackward0]
	2462638535200 -> 2462638535008
	2462638535200 [label=CudnnBatchNormBackward0]
	2462638535296 -> 2462638535200
	2462638535296 [label=ConvolutionBackward0]
	2462638535488 -> 2462638535296
	2462638535488 [label=MaxPool2DWithIndicesBackward0]
	2462638535680 -> 2462638535488
	2462638535680 [label=ReluBackward0]
	2462638535776 -> 2462638535680
	2462638535776 [label=CudnnBatchNormBackward0]
	2462638535872 -> 2462638535776
	2462638535872 [label=ConvolutionBackward0]
	2462638536064 -> 2462638535872
	2462638536064 [label=ReluBackward0]
	2462638536256 -> 2462638536064
	2462638536256 [label=CudnnBatchNormBackward0]
	2462638536352 -> 2462638536256
	2462638536352 [label=ConvolutionBackward0]
	2462638536544 -> 2462638536352
	2462638536544 [label=ReluBackward0]
	2462638536736 -> 2462638536544
	2462638536736 [label=CudnnBatchNormBackward0]
	2462638536832 -> 2462638536736
	2462638536832 [label=ConvolutionBackward0]
	2462638537024 -> 2462638536832
	2462638537024 [label=MaxPool2DWithIndicesBackward0]
	2462638537216 -> 2462638537024
	2462638537216 [label=ReluBackward0]
	2462638537312 -> 2462638537216
	2462638537312 [label=CudnnBatchNormBackward0]
	2462638537408 -> 2462638537312
	2462638537408 [label=ConvolutionBackward0]
	2462638537600 -> 2462638537408
	2462638537600 [label=ReluBackward0]
	2462638537792 -> 2462638537600
	2462638537792 [label=CudnnBatchNormBackward0]
	2462638537888 -> 2462638537792
	2462638537888 [label=ConvolutionBackward0]
	2462638538080 -> 2462638537888
	2462638538080 [label=ReluBackward0]
	2462638538272 -> 2462638538080
	2462638538272 [label=CudnnBatchNormBackward0]
	2462638538368 -> 2462638538272
	2462638538368 [label=ConvolutionBackward0]
	2462638538560 -> 2462638538368
	2462638538560 [label=MaxPool2DWithIndicesBackward0]
	2462638538704 -> 2462638538560
	2462638538704 [label=ReluBackward0]
	2462638948512 -> 2462638538704
	2462638948512 [label=CudnnBatchNormBackward0]
	2462638948608 -> 2462638948512
	2462638948608 [label=ConvolutionBackward0]
	2462638948800 -> 2462638948608
	2462638948800 [label=ReluBackward0]
	2462638948992 -> 2462638948800
	2462638948992 [label=CudnnBatchNormBackward0]
	2462638949088 -> 2462638948992
	2462638949088 [label=ConvolutionBackward0]
	2462638949280 -> 2462638949088
	2462638949280 [label=MaxPool2DWithIndicesBackward0]
	2462638949472 -> 2462638949280
	2462638949472 [label=ReluBackward0]
	2462638949568 -> 2462638949472
	2462638949568 [label=CudnnBatchNormBackward0]
	2462638949664 -> 2462638949568
	2462638949664 [label=ConvolutionBackward0]
	2462638949856 -> 2462638949664
	2462638949856 [label=ReluBackward0]
	2462638950048 -> 2462638949856
	2462638950048 [label=CudnnBatchNormBackward0]
	2462638950144 -> 2462638950048
	2462638950144 [label=ConvolutionBackward0]
	2462638950336 -> 2462638950144
	2462384676576 [label="encoderBlock1A.0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	2462384676576 -> 2462638950336
	2462638950336 [label=AccumulateGrad]
	2462638950288 -> 2462638950144
	2462384676256 [label="encoderBlock1A.0.bias
 (64)" fillcolor=lightblue]
	2462384676256 -> 2462638950288
	2462638950288 [label=AccumulateGrad]
	2462638950096 -> 2462638950048
	2462384676416 [label="encoderBlock1A.1.weight
 (64)" fillcolor=lightblue]
	2462384676416 -> 2462638950096
	2462638950096 [label=AccumulateGrad]
	2462638949952 -> 2462638950048
	2462384676336 [label="encoderBlock1A.1.bias
 (64)" fillcolor=lightblue]
	2462384676336 -> 2462638949952
	2462638949952 [label=AccumulateGrad]
	2462638949808 -> 2462638949664
	2462384675856 [label="encoderBlock1B.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2462384675856 -> 2462638949808
	2462638949808 [label=AccumulateGrad]
	2462638949760 -> 2462638949664
	2462384675536 [label="encoderBlock1B.0.bias
 (64)" fillcolor=lightblue]
	2462384675536 -> 2462638949760
	2462638949760 [label=AccumulateGrad]
	2462638949616 -> 2462638949568
	2462384675696 [label="encoderBlock1B.1.weight
 (64)" fillcolor=lightblue]
	2462384675696 -> 2462638949616
	2462638949616 [label=AccumulateGrad]
	2462638949376 -> 2462638949568
	2462384675616 [label="encoderBlock1B.1.bias
 (64)" fillcolor=lightblue]
	2462384675616 -> 2462638949376
	2462638949376 [label=AccumulateGrad]
	2462638949232 -> 2462638949088
	2462384680816 [label="encoderBlock2A.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2462384680816 -> 2462638949232
	2462638949232 [label=AccumulateGrad]
	2462638949184 -> 2462638949088
	2462384673136 [label="encoderBlock2A.0.bias
 (128)" fillcolor=lightblue]
	2462384673136 -> 2462638949184
	2462638949184 [label=AccumulateGrad]
	2462638949040 -> 2462638948992
	2462384673056 [label="encoderBlock2A.1.weight
 (128)" fillcolor=lightblue]
	2462384673056 -> 2462638949040
	2462638949040 [label=AccumulateGrad]
	2462638948896 -> 2462638948992
	2462384672736 [label="encoderBlock2A.1.bias
 (128)" fillcolor=lightblue]
	2462384672736 -> 2462638948896
	2462638948896 [label=AccumulateGrad]
	2462638948752 -> 2462638948608
	2462384672496 [label="encoderBlock2B.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2462384672496 -> 2462638948752
	2462638948752 [label=AccumulateGrad]
	2462638948704 -> 2462638948608
	2462384672416 [label="encoderBlock2B.0.bias
 (128)" fillcolor=lightblue]
	2462384672416 -> 2462638948704
	2462638948704 [label=AccumulateGrad]
	2462638948560 -> 2462638948512
	2462384675296 [label="encoderBlock2B.1.weight
 (128)" fillcolor=lightblue]
	2462384675296 -> 2462638948560
	2462638948560 [label=AccumulateGrad]
	2462638948416 -> 2462638948512
	2462384675456 [label="encoderBlock2B.1.bias
 (128)" fillcolor=lightblue]
	2462384675456 -> 2462638948416
	2462638948416 [label=AccumulateGrad]
	2462638538512 -> 2462638538368
	2462384671056 [label="encoderBlock3A.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2462384671056 -> 2462638538512
	2462638538512 [label=AccumulateGrad]
	2462638538464 -> 2462638538368
	2462384670976 [label="encoderBlock3A.0.bias
 (256)" fillcolor=lightblue]
	2462384670976 -> 2462638538464
	2462638538464 [label=AccumulateGrad]
	2462638538320 -> 2462638538272
	2462384670656 [label="encoderBlock3A.1.weight
 (256)" fillcolor=lightblue]
	2462384670656 -> 2462638538320
	2462638538320 [label=AccumulateGrad]
	2462638538176 -> 2462638538272
	2462384670816 [label="encoderBlock3A.1.bias
 (256)" fillcolor=lightblue]
	2462384670816 -> 2462638538176
	2462638538176 [label=AccumulateGrad]
	2462638538032 -> 2462638537888
	2462384670256 [label="encoderBlock3B.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462384670256 -> 2462638538032
	2462638538032 [label=AccumulateGrad]
	2462638537984 -> 2462638537888
	2462384671856 [label="encoderBlock3B.0.bias
 (256)" fillcolor=lightblue]
	2462384671856 -> 2462638537984
	2462638537984 [label=AccumulateGrad]
	2462638537840 -> 2462638537792
	2462384672016 [label="encoderBlock3B.1.weight
 (256)" fillcolor=lightblue]
	2462384672016 -> 2462638537840
	2462638537840 [label=AccumulateGrad]
	2462638537696 -> 2462638537792
	2462384671936 [label="encoderBlock3B.1.bias
 (256)" fillcolor=lightblue]
	2462384671936 -> 2462638537696
	2462638537696 [label=AccumulateGrad]
	2462638537552 -> 2462638537408
	2462384681536 [label="encoderBlock3B.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462384681536 -> 2462638537552
	2462638537552 [label=AccumulateGrad]
	2462638537504 -> 2462638537408
	2462384764976 [label="encoderBlock3B.3.bias
 (256)" fillcolor=lightblue]
	2462384764976 -> 2462638537504
	2462638537504 [label=AccumulateGrad]
	2462638537360 -> 2462638537312
	2462384764896 [label="encoderBlock3B.4.weight
 (256)" fillcolor=lightblue]
	2462384764896 -> 2462638537360
	2462638537360 [label=AccumulateGrad]
	2462638537120 -> 2462638537312
	2462384764816 [label="encoderBlock3B.4.bias
 (256)" fillcolor=lightblue]
	2462384764816 -> 2462638537120
	2462638537120 [label=AccumulateGrad]
	2462638536976 -> 2462638536832
	2462384764256 [label="encoderBlock4A.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2462384764256 -> 2462638536976
	2462638536976 [label=AccumulateGrad]
	2462638536928 -> 2462638536832
	2462384764176 [label="encoderBlock4A.0.bias
 (512)" fillcolor=lightblue]
	2462384764176 -> 2462638536928
	2462638536928 [label=AccumulateGrad]
	2462638536784 -> 2462638536736
	2462384764096 [label="encoderBlock4A.1.weight
 (512)" fillcolor=lightblue]
	2462384764096 -> 2462638536784
	2462638536784 [label=AccumulateGrad]
	2462638536640 -> 2462638536736
	2462384764016 [label="encoderBlock4A.1.bias
 (512)" fillcolor=lightblue]
	2462384764016 -> 2462638536640
	2462638536640 [label=AccumulateGrad]
	2462638536496 -> 2462638536352
	2462384763536 [label="encoderBlock4B.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462384763536 -> 2462638536496
	2462638536496 [label=AccumulateGrad]
	2462638536448 -> 2462638536352
	2462384763456 [label="encoderBlock4B.0.bias
 (512)" fillcolor=lightblue]
	2462384763456 -> 2462638536448
	2462638536448 [label=AccumulateGrad]
	2462638536304 -> 2462638536256
	2462384763376 [label="encoderBlock4B.1.weight
 (512)" fillcolor=lightblue]
	2462384763376 -> 2462638536304
	2462638536304 [label=AccumulateGrad]
	2462638536160 -> 2462638536256
	2462384763296 [label="encoderBlock4B.1.bias
 (512)" fillcolor=lightblue]
	2462384763296 -> 2462638536160
	2462638536160 [label=AccumulateGrad]
	2462638536016 -> 2462638535872
	2462384762816 [label="encoderBlock4B.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462384762816 -> 2462638536016
	2462638536016 [label=AccumulateGrad]
	2462638535968 -> 2462638535872
	2462384762736 [label="encoderBlock4B.3.bias
 (512)" fillcolor=lightblue]
	2462384762736 -> 2462638535968
	2462638535968 [label=AccumulateGrad]
	2462638535824 -> 2462638535776
	2462384762656 [label="encoderBlock4B.4.weight
 (512)" fillcolor=lightblue]
	2462384762656 -> 2462638535824
	2462638535824 [label=AccumulateGrad]
	2462638535584 -> 2462638535776
	2462384762576 [label="encoderBlock4B.4.bias
 (512)" fillcolor=lightblue]
	2462384762576 -> 2462638535584
	2462638535584 [label=AccumulateGrad]
	2462638535440 -> 2462638535296
	2462384761936 [label="encoderBlock5A.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462384761936 -> 2462638535440
	2462638535440 [label=AccumulateGrad]
	2462638535392 -> 2462638535296
	2462384761856 [label="encoderBlock5A.0.bias
 (512)" fillcolor=lightblue]
	2462384761856 -> 2462638535392
	2462638535392 [label=AccumulateGrad]
	2462638535248 -> 2462638535200
	2462384761776 [label="encoderBlock5A.1.weight
 (512)" fillcolor=lightblue]
	2462384761776 -> 2462638535248
	2462638535248 [label=AccumulateGrad]
	2462638535104 -> 2462638535200
	2462384761696 [label="encoderBlock5A.1.bias
 (512)" fillcolor=lightblue]
	2462384761696 -> 2462638535104
	2462638535104 [label=AccumulateGrad]
	2462638534960 -> 2462638534816
	2462384761216 [label="encoderBlock5B.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462384761216 -> 2462638534960
	2462638534960 [label=AccumulateGrad]
	2462638534912 -> 2462638534816
	2462384761136 [label="encoderBlock5B.0.bias
 (512)" fillcolor=lightblue]
	2462384761136 -> 2462638534912
	2462638534912 [label=AccumulateGrad]
	2462638534768 -> 2462638534720
	2462384762416 [label="encoderBlock5B.1.weight
 (512)" fillcolor=lightblue]
	2462384762416 -> 2462638534768
	2462638534768 [label=AccumulateGrad]
	2462638534624 -> 2462638534720
	2462384762336 [label="encoderBlock5B.1.bias
 (512)" fillcolor=lightblue]
	2462384762336 -> 2462638534624
	2462638534624 [label=AccumulateGrad]
	2462638534480 -> 2462638534336
	2462384760656 [label="encoderBlock5B.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462384760656 -> 2462638534480
	2462638534480 [label=AccumulateGrad]
	2462638534432 -> 2462638534336
	2462384760416 [label="encoderBlock5B.3.bias
 (512)" fillcolor=lightblue]
	2462384760416 -> 2462638534432
	2462638534432 [label=AccumulateGrad]
	2462638534288 -> 2462638534240
	2462384760336 [label="encoderBlock5B.4.weight
 (512)" fillcolor=lightblue]
	2462384760336 -> 2462638534288
	2462638534288 [label=AccumulateGrad]
	2462638533952 -> 2462638534240
	2462384760576 [label="encoderBlock5B.4.bias
 (512)" fillcolor=lightblue]
	2462384760576 -> 2462638533952
	2462638533952 [label=AccumulateGrad]
	2462638533808 -> 2462638533664
	2462534221504 [label="decoderBlock5.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462534221504 -> 2462638533808
	2462638533808 [label=AccumulateGrad]
	2462638533760 -> 2462638533664
	2462534221424 [label="decoderBlock5.0.bias
 (512)" fillcolor=lightblue]
	2462534221424 -> 2462638533760
	2462638533760 [label=AccumulateGrad]
	2462638533616 -> 2462638533568
	2462534221264 [label="decoderBlock5.1.weight
 (512)" fillcolor=lightblue]
	2462534221264 -> 2462638533616
	2462638533616 [label=AccumulateGrad]
	2462638533472 -> 2462638533568
	2462534221184 [label="decoderBlock5.1.bias
 (512)" fillcolor=lightblue]
	2462534221184 -> 2462638533472
	2462638533472 [label=AccumulateGrad]
	2462638533328 -> 2462638533184
	2462534220544 [label="decoderBlock5.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462534220544 -> 2462638533328
	2462638533328 [label=AccumulateGrad]
	2462638533280 -> 2462638533184
	2462534220464 [label="decoderBlock5.3.bias
 (512)" fillcolor=lightblue]
	2462534220464 -> 2462638533280
	2462638533280 [label=AccumulateGrad]
	2462638533136 -> 2462638533088
	2462534220304 [label="decoderBlock5.4.weight
 (512)" fillcolor=lightblue]
	2462534220304 -> 2462638533136
	2462638533136 [label=AccumulateGrad]
	2462638532992 -> 2462638533088
	2462534220224 [label="decoderBlock5.4.bias
 (512)" fillcolor=lightblue]
	2462534220224 -> 2462638532992
	2462638532992 [label=AccumulateGrad]
	2462638532848 -> 2462638532704
	2462534219424 [label="decoderBlock5.6.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462534219424 -> 2462638532848
	2462638532848 [label=AccumulateGrad]
	2462638532800 -> 2462638532704
	2462534219344 [label="decoderBlock5.6.bias
 (512)" fillcolor=lightblue]
	2462534219344 -> 2462638532800
	2462638532800 [label=AccumulateGrad]
	2462638532656 -> 2462638532608
	2462534219184 [label="decoderBlock5.7.weight
 (512)" fillcolor=lightblue]
	2462534219184 -> 2462638532656
	2462638532656 [label=AccumulateGrad]
	2462638532416 -> 2462638532608
	2462534219104 [label="decoderBlock5.7.bias
 (512)" fillcolor=lightblue]
	2462534219104 -> 2462638532416
	2462638532416 [label=AccumulateGrad]
	2462638532272 -> 2462638532128
	2462534218464 [label="decoderBlock4.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462534218464 -> 2462638532272
	2462638532272 [label=AccumulateGrad]
	2462638532224 -> 2462638532128
	2462534218384 [label="decoderBlock4.0.bias
 (512)" fillcolor=lightblue]
	2462534218384 -> 2462638532224
	2462638532224 [label=AccumulateGrad]
	2462638532080 -> 2462638532032
	2462534218224 [label="decoderBlock4.1.weight
 (512)" fillcolor=lightblue]
	2462534218224 -> 2462638532080
	2462638532080 [label=AccumulateGrad]
	2462638531936 -> 2462638532032
	2462534218144 [label="decoderBlock4.1.bias
 (512)" fillcolor=lightblue]
	2462534218144 -> 2462638531936
	2462638531936 [label=AccumulateGrad]
	2462638531792 -> 2462638531648
	2462534217344 [label="decoderBlock4.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462534217344 -> 2462638531792
	2462638531792 [label=AccumulateGrad]
	2462638531744 -> 2462638531648
	2462534217264 [label="decoderBlock4.3.bias
 (512)" fillcolor=lightblue]
	2462534217264 -> 2462638531744
	2462638531744 [label=AccumulateGrad]
	2462638531600 -> 2462638531552
	2462534217104 [label="decoderBlock4.4.weight
 (512)" fillcolor=lightblue]
	2462534217104 -> 2462638531600
	2462638531600 [label=AccumulateGrad]
	2462638531456 -> 2462638531552
	2462534217024 [label="decoderBlock4.4.bias
 (512)" fillcolor=lightblue]
	2462534217024 -> 2462638531456
	2462638531456 [label=AccumulateGrad]
	2462638531312 -> 2462638531168
	2462534216384 [label="decoderBlock4.6.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	2462534216384 -> 2462638531312
	2462638531312 [label=AccumulateGrad]
	2462638531264 -> 2462638531168
	2462534216304 [label="decoderBlock4.6.bias
 (256)" fillcolor=lightblue]
	2462534216304 -> 2462638531264
	2462638531264 [label=AccumulateGrad]
	2462638531120 -> 2462638531072
	2462534216144 [label="decoderBlock4.7.weight
 (256)" fillcolor=lightblue]
	2462534216144 -> 2462638531120
	2462638531120 [label=AccumulateGrad]
	2462638530880 -> 2462638531072
	2462534216064 [label="decoderBlock4.7.bias
 (256)" fillcolor=lightblue]
	2462534216064 -> 2462638530880
	2462638530880 [label=AccumulateGrad]
	2462638530736 -> 2462638530592
	2462534215264 [label="decoderBlock3.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462534215264 -> 2462638530736
	2462638530736 [label=AccumulateGrad]
	2462638530688 -> 2462638530592
	2462534215184 [label="decoderBlock3.0.bias
 (256)" fillcolor=lightblue]
	2462534215184 -> 2462638530688
	2462638530688 [label=AccumulateGrad]
	2462638530544 -> 2462638530496
	2462534215024 [label="decoderBlock3.1.weight
 (256)" fillcolor=lightblue]
	2462534215024 -> 2462638530544
	2462638530544 [label=AccumulateGrad]
	2462638530400 -> 2462638530496
	2462534214944 [label="decoderBlock3.1.bias
 (256)" fillcolor=lightblue]
	2462534214944 -> 2462638530400
	2462638530400 [label=AccumulateGrad]
	2462638530256 -> 2462638530112
	2462534214384 [label="decoderBlock3.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2462534214384 -> 2462638530256
	2462638530256 [label=AccumulateGrad]
	2462638530208 -> 2462638530112
	2462534214304 [label="decoderBlock3.3.bias
 (256)" fillcolor=lightblue]
	2462534214304 -> 2462638530208
	2462638530208 [label=AccumulateGrad]
	2462638530064 -> 2462638530016
	2462534214224 [label="decoderBlock3.4.weight
 (256)" fillcolor=lightblue]
	2462534214224 -> 2462638530064
	2462638530064 [label=AccumulateGrad]
	2462638529920 -> 2462638530016
	2462534214064 [label="decoderBlock3.4.bias
 (256)" fillcolor=lightblue]
	2462534214064 -> 2462638529920
	2462638529920 [label=AccumulateGrad]
	2462638529776 -> 2462638529632
	2462534213264 [label="decoderBlock3.6.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	2462534213264 -> 2462638529776
	2462638529776 [label=AccumulateGrad]
	2462638529728 -> 2462638529632
	2462534213184 [label="decoderBlock3.6.bias
 (128)" fillcolor=lightblue]
	2462534213184 -> 2462638529728
	2462638529728 [label=AccumulateGrad]
	2462638529584 -> 2462638529536
	2462534213104 [label="decoderBlock3.7.weight
 (128)" fillcolor=lightblue]
	2462534213104 -> 2462638529584
	2462638529584 [label=AccumulateGrad]
	2462638529344 -> 2462638529536
	2462534212944 [label="decoderBlock3.7.bias
 (128)" fillcolor=lightblue]
	2462534212944 -> 2462638529344
	2462638529344 [label=AccumulateGrad]
	2462638529200 -> 2462638529056
	2462534212304 [label="decoderBlock2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2462534212304 -> 2462638529200
	2462638529200 [label=AccumulateGrad]
	2462638529152 -> 2462638529056
	2462534212144 [label="decoderBlock2.0.bias
 (128)" fillcolor=lightblue]
	2462534212144 -> 2462638529152
	2462638529152 [label=AccumulateGrad]
	2462638529008 -> 2462638528960
	2462534212064 [label="decoderBlock2.1.weight
 (128)" fillcolor=lightblue]
	2462534212064 -> 2462638529008
	2462638529008 [label=AccumulateGrad]
	2462638528864 -> 2462638528960
	2462534211984 [label="decoderBlock2.1.bias
 (128)" fillcolor=lightblue]
	2462534211984 -> 2462638528864
	2462638528864 [label=AccumulateGrad]
	2462638528720 -> 2462638528576
	2462534211184 [label="decoderBlock2.3.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	2462534211184 -> 2462638528720
	2462638528720 [label=AccumulateGrad]
	2462638528672 -> 2462638528576
	2462534211024 [label="decoderBlock2.3.bias
 (64)" fillcolor=lightblue]
	2462534211024 -> 2462638528672
	2462638528672 [label=AccumulateGrad]
	2462638528528 -> 2462638528480
	2462534210944 [label="decoderBlock2.4.weight
 (64)" fillcolor=lightblue]
	2462534210944 -> 2462638528528
	2462638528528 [label=AccumulateGrad]
	2462638528288 -> 2462638528480
	2462534210864 [label="decoderBlock2.4.bias
 (64)" fillcolor=lightblue]
	2462534210864 -> 2462638528288
	2462638528288 [label=AccumulateGrad]
	2462638528192 -> 2462638528048
	2462638528192 [label=ReluBackward0]
	2462638528624 -> 2462638528192
	2462638528624 [label=CudnnBatchNormBackward0]
	2462638528912 -> 2462638528624
	2462638528912 [label=ConvolutionBackward0]
	2462638529488 -> 2462638528912
	2462638529488 [label=UpsampleBilinear2DBackward0]
	2462638530160 -> 2462638529488
	2462638530160 [label=MulBackward0]
	2462638530448 -> 2462638530160
	2462638530448 [label=SigmoidBackward0]
	2462638530832 -> 2462638530448
	2462638530832 [label=CudnnBatchNormBackward0]
	2462638531024 -> 2462638530832
	2462638531024 [label=ConvolutionBackward0]
	2462638531696 -> 2462638531024
	2462638531696 [label=ReluBackward0]
	2462638532368 -> 2462638531696
	2462638532368 [label=CudnnBatchNormBackward0]
	2462638532464 -> 2462638532368
	2462638532464 [label=ConvolutionBackward0]
	2462638533424 -> 2462638532464
	2462638533424 [label=CatBackward0]
	2462638529248 -> 2462638533424
	2462638534000 -> 2462638533424
	2462638534000 [label=ReluBackward0]
	2462638534192 -> 2462638534000
	2462638534192 [label=CudnnBatchNormBackward0]
	2462638534576 -> 2462638534192
	2462638534576 [label=ConvolutionBackward0]
	2462638535152 -> 2462638534576
	2462638535152 [label=UpsampleBilinear2DBackward0]
	2462638535728 -> 2462638535152
	2462638535728 [label=MulBackward0]
	2462638536112 -> 2462638535728
	2462638536112 [label=SigmoidBackward0]
	2462638536400 -> 2462638536112
	2462638536400 [label=CudnnBatchNormBackward0]
	2462638536688 -> 2462638536400
	2462638536688 [label=ConvolutionBackward0]
	2462638537264 -> 2462638536688
	2462638537264 [label=ReluBackward0]
	2462638537936 -> 2462638537264
	2462638537936 [label=CudnnBatchNormBackward0]
	2462638538224 -> 2462638537936
	2462638538224 [label=ConvolutionBackward0]
	2462638538656 -> 2462638538224
	2462638538656 [label=CatBackward0]
	2462638530784 -> 2462638538656
	2462638949136 -> 2462638538656
	2462638949136 [label=ReluBackward0]
	2462638949424 -> 2462638949136
	2462638949424 [label=CudnnBatchNormBackward0]
	2462638949712 -> 2462638949424
	2462638949712 [label=ConvolutionBackward0]
	2462638950240 -> 2462638949712
	2462638950240 [label=UpsampleBilinear2DBackward0]
	2462638950528 -> 2462638950240
	2462638950528 [label=MulBackward0]
	2462638950624 -> 2462638950528
	2462638950624 [label=SigmoidBackward0]
	2462638950720 -> 2462638950624
	2462638950720 [label=CudnnBatchNormBackward0]
	2462638950816 -> 2462638950720
	2462638950816 [label=ConvolutionBackward0]
	2462638951008 -> 2462638950816
	2462638951008 [label=ReluBackward0]
	2462638951200 -> 2462638951008
	2462638951200 [label=CudnnBatchNormBackward0]
	2462638951296 -> 2462638951200
	2462638951296 [label=ConvolutionBackward0]
	2462638951488 -> 2462638951296
	2462638951488 [label=CatBackward0]
	2462638532320 -> 2462638951488
	2462638951680 -> 2462638951488
	2462638951680 [label=ReluBackward0]
	2462638951776 -> 2462638951680
	2462638951776 [label=CudnnBatchNormBackward0]
	2462638951872 -> 2462638951776
	2462638951872 [label=ConvolutionBackward0]
	2462638952064 -> 2462638951872
	2462638952064 [label=UpsampleBilinear2DBackward0]
	2462638952256 -> 2462638952064
	2462638952256 [label=MulBackward0]
	2462638952352 -> 2462638952256
	2462638952352 [label=SigmoidBackward0]
	2462638952448 -> 2462638952352
	2462638952448 [label=CudnnBatchNormBackward0]
	2462638952544 -> 2462638952448
	2462638952544 [label=ConvolutionBackward0]
	2462638952736 -> 2462638952544
	2462638952736 [label=ReluBackward0]
	2462638952928 -> 2462638952736
	2462638952928 [label=CudnnBatchNormBackward0]
	2462638953024 -> 2462638952928
	2462638953024 [label=ConvolutionBackward0]
	2462638953216 -> 2462638953024
	2462638953216 [label=CatBackward0]
	2462638533856 -> 2462638953216
	2462638953408 -> 2462638953216
	2462638953408 [label=ReluBackward0]
	2462638953504 -> 2462638953408
	2462638953504 [label=CudnnBatchNormBackward0]
	2462638953600 -> 2462638953504
	2462638953600 [label=ConvolutionBackward0]
	2462638953792 -> 2462638953600
	2462638953792 [label=UpsampleBilinear2DBackward0]
	2462638953984 -> 2462638953792
	2462638953984 [label=MaxPool2DWithIndicesBackward0]
	2462638954080 -> 2462638953984
	2462638954080 [label=ReluBackward0]
	2462638954176 -> 2462638954080
	2462638954176 [label=CudnnBatchNormBackward0]
	2462638954272 -> 2462638954176
	2462638954272 [label=ConvolutionBackward0]
	2462638954464 -> 2462638954272
	2462638954464 [label=MulBackward0]
	2462638954656 -> 2462638954464
	2462638954656 [label=SigmoidBackward0]
	2462638954752 -> 2462638954656
	2462638954752 [label=CudnnBatchNormBackward0]
	2462638954848 -> 2462638954752
	2462638954848 [label=ConvolutionBackward0]
	2462638955040 -> 2462638954848
	2462638955040 [label=ReluBackward0]
	2462638955232 -> 2462638955040
	2462638955232 [label=CudnnBatchNormBackward0]
	2462638955328 -> 2462638955232
	2462638955328 [label=ConvolutionBackward0]
	2462638955520 -> 2462638955328
	2462638955520 [label=CatBackward0]
	2462638535008 -> 2462638955520
	2462638955712 -> 2462638955520
	2462638955712 [label=MaxPool2DWithIndicesBackward0]
	2462638955808 -> 2462638955712
	2462638955808 [label=ReluBackward0]
	2462638955904 -> 2462638955808
	2462638955904 [label=CudnnBatchNormBackward0]
	2462638956000 -> 2462638955904
	2462638956000 [label=ConvolutionBackward0]
	2462638956192 -> 2462638956000
	2462638956192 [label=MulBackward0]
	2462638956384 -> 2462638956192
	2462638956384 [label=SigmoidBackward0]
	2462638956480 -> 2462638956384
	2462638956480 [label=CudnnBatchNormBackward0]
	2462638956576 -> 2462638956480
	2462638956576 [label=ConvolutionBackward0]
	2462638956768 -> 2462638956576
	2462638956768 [label=ReluBackward0]
	2462638956960 -> 2462638956768
	2462638956960 [label=CudnnBatchNormBackward0]
	2462638957056 -> 2462638956960
	2462638957056 [label=ConvolutionBackward0]
	2462638957248 -> 2462638957056
	2462638957248 [label=CatBackward0]
	2462638536544 -> 2462638957248
	2462638957440 -> 2462638957248
	2462638957440 [label=MaxPool2DWithIndicesBackward0]
	2462638957536 -> 2462638957440
	2462638957536 [label=ReluBackward0]
	2462638957632 -> 2462638957536
	2462638957632 [label=CudnnBatchNormBackward0]
	2462638957728 -> 2462638957632
	2462638957728 [label=ConvolutionBackward0]
	2462638957920 -> 2462638957728
	2462638957920 [label=MulBackward0]
	2462638958112 -> 2462638957920
	2462638958112 [label=SigmoidBackward0]
	2462638958208 -> 2462638958112
	2462638958208 [label=CudnnBatchNormBackward0]
	2462638958304 -> 2462638958208
	2462638958304 [label=ConvolutionBackward0]
	2462638958496 -> 2462638958304
	2462638958496 [label=ReluBackward0]
	2462638958688 -> 2462638958496
	2462638958688 [label=CudnnBatchNormBackward0]
	2462638958784 -> 2462638958688
	2462638958784 [label=ConvolutionBackward0]
	2462638958976 -> 2462638958784
	2462638958976 [label=CatBackward0]
	2462638538080 -> 2462638958976
	2462638959168 -> 2462638958976
	2462638959168 [label=MaxPool2DWithIndicesBackward0]
	2462638959264 -> 2462638959168
	2462638959264 [label=ReluBackward0]
	2462638959360 -> 2462638959264
	2462638959360 [label=CudnnBatchNormBackward0]
	2462638959456 -> 2462638959360
	2462638959456 [label=ConvolutionBackward0]
	2462638959648 -> 2462638959456
	2462638959648 [label=MulBackward0]
	2462638959840 -> 2462638959648
	2462638959840 [label=SigmoidBackward0]
	2462638959936 -> 2462638959840
	2462638959936 [label=CudnnBatchNormBackward0]
	2462638960032 -> 2462638959936
	2462638960032 [label=ConvolutionBackward0]
	2462638960224 -> 2462638960032
	2462638960224 [label=ReluBackward0]
	2462638960416 -> 2462638960224
	2462638960416 [label=CudnnBatchNormBackward0]
	2462638960512 -> 2462638960416
	2462638960512 [label=ConvolutionBackward0]
	2462638960704 -> 2462638960512
	2462638960704 [label=CatBackward0]
	2462638948800 -> 2462638960704
	2462638960896 -> 2462638960704
	2462638960896 [label=MaxPool2DWithIndicesBackward0]
	2462638960992 -> 2462638960896
	2462638960992 [label=ReluBackward0]
	2462638961088 -> 2462638960992
	2462638961088 [label=CudnnBatchNormBackward0]
	2462638961184 -> 2462638961088
	2462638961184 [label=ConvolutionBackward0]
	2462638961376 -> 2462638961184
	2462638961376 [label=MulBackward0]
	2462638961568 -> 2462638961376
	2462638961568 [label=SigmoidBackward0]
	2462638961664 -> 2462638961568
	2462638961664 [label=CudnnBatchNormBackward0]
	2462638961760 -> 2462638961664
	2462638961760 [label=ConvolutionBackward0]
	2462638961952 -> 2462638961760
	2462638961952 [label=ReluBackward0]
	2462638962144 -> 2462638961952
	2462638962144 [label=CudnnBatchNormBackward0]
	2462638962240 -> 2462638962144
	2462638962240 [label=ConvolutionBackward0]
	2462638949856 -> 2462638962240
	2462638962432 -> 2462638962240
	2462534208144 [label="encoderAttentionModule1.0.0.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2462534208144 -> 2462638962432
	2462638962432 [label=AccumulateGrad]
	2462638962384 -> 2462638962240
	2462534207984 [label="encoderAttentionModule1.0.0.bias
 (64)" fillcolor=lightblue]
	2462534207984 -> 2462638962384
	2462638962384 [label=AccumulateGrad]
	2462638962192 -> 2462638962144
	2462534207904 [label="encoderAttentionModule1.0.1.weight
 (64)" fillcolor=lightblue]
	2462534207904 -> 2462638962192
	2462638962192 [label=AccumulateGrad]
	2462638962048 -> 2462638962144
	2462534207824 [label="encoderAttentionModule1.0.1.bias
 (64)" fillcolor=lightblue]
	2462534207824 -> 2462638962048
	2462638962048 [label=AccumulateGrad]
	2462638961904 -> 2462638961760
	2462534207024 [label="encoderAttentionModule1.0.3.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2462534207024 -> 2462638961904
	2462638961904 [label=AccumulateGrad]
	2462638961856 -> 2462638961760
	2462534206864 [label="encoderAttentionModule1.0.3.bias
 (64)" fillcolor=lightblue]
	2462534206864 -> 2462638961856
	2462638961856 [label=AccumulateGrad]
	2462638961712 -> 2462638961664
	2462534206784 [label="encoderAttentionModule1.0.4.weight
 (64)" fillcolor=lightblue]
	2462534206784 -> 2462638961712
	2462638961712 [label=AccumulateGrad]
	2462638961472 -> 2462638961664
	2462534206704 [label="encoderAttentionModule1.0.4.bias
 (64)" fillcolor=lightblue]
	2462534206704 -> 2462638961472
	2462638961472 [label=AccumulateGrad]
	2462638949472 -> 2462638961376
	2462638961328 -> 2462638961184
	2462534206064 [label="encoderAttentionConv1.0.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2462534206064 -> 2462638961328
	2462638961328 [label=AccumulateGrad]
	2462638961280 -> 2462638961184
	2462534205904 [label="encoderAttentionConv1.0.0.bias
 (128)" fillcolor=lightblue]
	2462534205904 -> 2462638961280
	2462638961280 [label=AccumulateGrad]
	2462638961136 -> 2462638961088
	2462534205824 [label="encoderAttentionConv1.0.1.weight
 (128)" fillcolor=lightblue]
	2462534205824 -> 2462638961136
	2462638961136 [label=AccumulateGrad]
	2462638960800 -> 2462638961088
	2462534205744 [label="encoderAttentionConv1.0.1.bias
 (128)" fillcolor=lightblue]
	2462534205744 -> 2462638960800
	2462638960800 [label=AccumulateGrad]
	2462638960656 -> 2462638960512
	2462532959696 [label="encoderAttentionModule2.0.0.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2462532959696 -> 2462638960656
	2462638960656 [label=AccumulateGrad]
	2462638960608 -> 2462638960512
	2462532959536 [label="encoderAttentionModule2.0.0.bias
 (128)" fillcolor=lightblue]
	2462532959536 -> 2462638960608
	2462638960608 [label=AccumulateGrad]
	2462638960464 -> 2462638960416
	2462532959456 [label="encoderAttentionModule2.0.1.weight
 (128)" fillcolor=lightblue]
	2462532959456 -> 2462638960464
	2462638960464 [label=AccumulateGrad]
	2462638960320 -> 2462638960416
	2462532959376 [label="encoderAttentionModule2.0.1.bias
 (128)" fillcolor=lightblue]
	2462532959376 -> 2462638960320
	2462638960320 [label=AccumulateGrad]
	2462638960176 -> 2462638960032
	2462532958656 [label="encoderAttentionModule2.0.3.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2462532958656 -> 2462638960176
	2462638960176 [label=AccumulateGrad]
	2462638960128 -> 2462638960032
	2462532958576 [label="encoderAttentionModule2.0.3.bias
 (128)" fillcolor=lightblue]
	2462532958576 -> 2462638960128
	2462638960128 [label=AccumulateGrad]
	2462638959984 -> 2462638959936
	2462532958416 [label="encoderAttentionModule2.0.4.weight
 (128)" fillcolor=lightblue]
	2462532958416 -> 2462638959984
	2462638959984 [label=AccumulateGrad]
	2462638959744 -> 2462638959936
	2462532958176 [label="encoderAttentionModule2.0.4.bias
 (128)" fillcolor=lightblue]
	2462532958176 -> 2462638959744
	2462638959744 [label=AccumulateGrad]
	2462638538704 -> 2462638959648
	2462638959600 -> 2462638959456
	2462532957536 [label="encoderAttentionConv2.0.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2462532957536 -> 2462638959600
	2462638959600 [label=AccumulateGrad]
	2462638959552 -> 2462638959456
	2462532957456 [label="encoderAttentionConv2.0.0.bias
 (256)" fillcolor=lightblue]
	2462532957456 -> 2462638959552
	2462638959552 [label=AccumulateGrad]
	2462638959408 -> 2462638959360
	2462532957296 [label="encoderAttentionConv2.0.1.weight
 (256)" fillcolor=lightblue]
	2462532957296 -> 2462638959408
	2462638959408 [label=AccumulateGrad]
	2462638959072 -> 2462638959360
	2462532957216 [label="encoderAttentionConv2.0.1.bias
 (256)" fillcolor=lightblue]
	2462532957216 -> 2462638959072
	2462638959072 [label=AccumulateGrad]
	2462638958928 -> 2462638958784
	2462532956496 [label="encoderAttentionModule3.0.0.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2462532956496 -> 2462638958928
	2462638958928 [label=AccumulateGrad]
	2462638958880 -> 2462638958784
	2462532956256 [label="encoderAttentionModule3.0.0.bias
 (256)" fillcolor=lightblue]
	2462532956256 -> 2462638958880
	2462638958880 [label=AccumulateGrad]
	2462638958736 -> 2462638958688
	2462532956176 [label="encoderAttentionModule3.0.1.weight
 (256)" fillcolor=lightblue]
	2462532956176 -> 2462638958736
	2462638958736 [label=AccumulateGrad]
	2462638958592 -> 2462638958688
	2462532956016 [label="encoderAttentionModule3.0.1.bias
 (256)" fillcolor=lightblue]
	2462532956016 -> 2462638958592
	2462638958592 [label=AccumulateGrad]
	2462638958448 -> 2462638958304
	2462532955376 [label="encoderAttentionModule3.0.3.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2462532955376 -> 2462638958448
	2462638958448 [label=AccumulateGrad]
	2462638958400 -> 2462638958304
	2462532955296 [label="encoderAttentionModule3.0.3.bias
 (256)" fillcolor=lightblue]
	2462532955296 -> 2462638958400
	2462638958400 [label=AccumulateGrad]
	2462638958256 -> 2462638958208
	2462532955216 [label="encoderAttentionModule3.0.4.weight
 (256)" fillcolor=lightblue]
	2462532955216 -> 2462638958256
	2462638958256 [label=AccumulateGrad]
	2462638958016 -> 2462638958208
	2462532955136 [label="encoderAttentionModule3.0.4.bias
 (256)" fillcolor=lightblue]
	2462532955136 -> 2462638958016
	2462638958016 [label=AccumulateGrad]
	2462638537216 -> 2462638957920
	2462638957872 -> 2462638957728
	2462384751696 [label="encoderAttentionConv3.0.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2462384751696 -> 2462638957872
	2462638957872 [label=AccumulateGrad]
	2462638957824 -> 2462638957728
	2462384758336 [label="encoderAttentionConv3.0.0.bias
 (512)" fillcolor=lightblue]
	2462384758336 -> 2462638957824
	2462638957824 [label=AccumulateGrad]
	2462638957680 -> 2462638957632
	2462384758256 [label="encoderAttentionConv3.0.1.weight
 (512)" fillcolor=lightblue]
	2462384758256 -> 2462638957680
	2462638957680 [label=AccumulateGrad]
	2462638957344 -> 2462638957632
	2462384765056 [label="encoderAttentionConv3.0.1.bias
 (512)" fillcolor=lightblue]
	2462384765056 -> 2462638957344
	2462638957344 [label=AccumulateGrad]
	2462638957200 -> 2462638957056
	2462384758496 [label="encoderAttentionModule4.0.0.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2462384758496 -> 2462638957200
	2462638957200 [label=AccumulateGrad]
	2462638957152 -> 2462638957056
	2462384758976 [label="encoderAttentionModule4.0.0.bias
 (512)" fillcolor=lightblue]
	2462384758976 -> 2462638957152
	2462638957152 [label=AccumulateGrad]
	2462638957008 -> 2462638956960
	2462534244112 [label="encoderAttentionModule4.0.1.weight
 (512)" fillcolor=lightblue]
	2462534244112 -> 2462638957008
	2462638957008 [label=AccumulateGrad]
	2462638956864 -> 2462638956960
	2462534244032 [label="encoderAttentionModule4.0.1.bias
 (512)" fillcolor=lightblue]
	2462534244032 -> 2462638956864
	2462638956864 [label=AccumulateGrad]
	2462638956720 -> 2462638956576
	2462534243392 [label="encoderAttentionModule4.0.3.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	2462534243392 -> 2462638956720
	2462638956720 [label=AccumulateGrad]
	2462638956672 -> 2462638956576
	2462534243312 [label="encoderAttentionModule4.0.3.bias
 (512)" fillcolor=lightblue]
	2462534243312 -> 2462638956672
	2462638956672 [label=AccumulateGrad]
	2462638956528 -> 2462638956480
	2462534243152 [label="encoderAttentionModule4.0.4.weight
 (512)" fillcolor=lightblue]
	2462534243152 -> 2462638956528
	2462638956528 [label=AccumulateGrad]
	2462638956288 -> 2462638956480
	2462534243072 [label="encoderAttentionModule4.0.4.bias
 (512)" fillcolor=lightblue]
	2462534243072 -> 2462638956288
	2462638956288 [label=AccumulateGrad]
	2462638535680 -> 2462638956192
	2462638956144 -> 2462638956000
	2462534242432 [label="encoderAttentionConv4.0.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462534242432 -> 2462638956144
	2462638956144 [label=AccumulateGrad]
	2462638956096 -> 2462638956000
	2462534242352 [label="encoderAttentionConv4.0.0.bias
 (512)" fillcolor=lightblue]
	2462534242352 -> 2462638956096
	2462638956096 [label=AccumulateGrad]
	2462638955952 -> 2462638955904
	2462534242192 [label="encoderAttentionConv4.0.1.weight
 (512)" fillcolor=lightblue]
	2462534242192 -> 2462638955952
	2462638955952 [label=AccumulateGrad]
	2462638955616 -> 2462638955904
	2462534242112 [label="encoderAttentionConv4.0.1.bias
 (512)" fillcolor=lightblue]
	2462534242112 -> 2462638955616
	2462638955616 [label=AccumulateGrad]
	2462638955472 -> 2462638955328
	2462534241472 [label="encoderAttentionModule5.0.0.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2462534241472 -> 2462638955472
	2462638955472 [label=AccumulateGrad]
	2462638955424 -> 2462638955328
	2462534241392 [label="encoderAttentionModule5.0.0.bias
 (512)" fillcolor=lightblue]
	2462534241392 -> 2462638955424
	2462638955424 [label=AccumulateGrad]
	2462638955280 -> 2462638955232
	2462534241232 [label="encoderAttentionModule5.0.1.weight
 (512)" fillcolor=lightblue]
	2462534241232 -> 2462638955280
	2462638955280 [label=AccumulateGrad]
	2462638955136 -> 2462638955232
	2462534241152 [label="encoderAttentionModule5.0.1.bias
 (512)" fillcolor=lightblue]
	2462534241152 -> 2462638955136
	2462638955136 [label=AccumulateGrad]
	2462638954992 -> 2462638954848
	2462534240352 [label="encoderAttentionModule5.0.3.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	2462534240352 -> 2462638954992
	2462638954992 [label=AccumulateGrad]
	2462638954944 -> 2462638954848
	2462534240272 [label="encoderAttentionModule5.0.3.bias
 (512)" fillcolor=lightblue]
	2462534240272 -> 2462638954944
	2462638954944 [label=AccumulateGrad]
	2462638954800 -> 2462638954752
	2462534240112 [label="encoderAttentionModule5.0.4.weight
 (512)" fillcolor=lightblue]
	2462534240112 -> 2462638954800
	2462638954800 [label=AccumulateGrad]
	2462638954560 -> 2462638954752
	2462534240032 [label="encoderAttentionModule5.0.4.bias
 (512)" fillcolor=lightblue]
	2462534240032 -> 2462638954560
	2462638954560 [label=AccumulateGrad]
	2462638534144 -> 2462638954464
	2462638954416 -> 2462638954272
	2462534239392 [label="encoderAttentionConv5.0.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462534239392 -> 2462638954416
	2462638954416 [label=AccumulateGrad]
	2462638954368 -> 2462638954272
	2462534239312 [label="encoderAttentionConv5.0.0.bias
 (512)" fillcolor=lightblue]
	2462534239312 -> 2462638954368
	2462638954368 [label=AccumulateGrad]
	2462638954224 -> 2462638954176
	2462534239152 [label="encoderAttentionConv5.0.1.weight
 (512)" fillcolor=lightblue]
	2462534239152 -> 2462638954224
	2462638954224 [label=AccumulateGrad]
	2462638953888 -> 2462638954176
	2462534239072 [label="encoderAttentionConv5.0.1.bias
 (512)" fillcolor=lightblue]
	2462534239072 -> 2462638953888
	2462638953888 [label=AccumulateGrad]
	2462638953744 -> 2462638953600
	2462638345328 [label="decoderAttentionConv5.0.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2462638345328 -> 2462638953744
	2462638953744 [label=AccumulateGrad]
	2462638953696 -> 2462638953600
	2462638345408 [label="decoderAttentionConv5.0.0.bias
 (512)" fillcolor=lightblue]
	2462638345408 -> 2462638953696
	2462638953696 [label=AccumulateGrad]
	2462638953552 -> 2462638953504
	2462638345488 [label="decoderAttentionConv5.0.1.weight
 (512)" fillcolor=lightblue]
	2462638345488 -> 2462638953552
	2462638953552 [label=AccumulateGrad]
	2462638953312 -> 2462638953504
	2462638345568 [label="decoderAttentionConv5.0.1.bias
 (512)" fillcolor=lightblue]
	2462638345568 -> 2462638953312
	2462638953312 [label=AccumulateGrad]
	2462638953168 -> 2462638953024
	2462638343888 [label="decoderAttentionModule5.0.0.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2462638343888 -> 2462638953168
	2462638953168 [label=AccumulateGrad]
	2462638953120 -> 2462638953024
	2462638343968 [label="decoderAttentionModule5.0.0.bias
 (512)" fillcolor=lightblue]
	2462638343968 -> 2462638953120
	2462638953120 [label=AccumulateGrad]
	2462638952976 -> 2462638952928
	2462638344048 [label="decoderAttentionModule5.0.1.weight
 (512)" fillcolor=lightblue]
	2462638344048 -> 2462638952976
	2462638952976 [label=AccumulateGrad]
	2462638952832 -> 2462638952928
	2462638344128 [label="decoderAttentionModule5.0.1.bias
 (512)" fillcolor=lightblue]
	2462638344128 -> 2462638952832
	2462638952832 [label=AccumulateGrad]
	2462638952688 -> 2462638952544
	2462638344608 [label="decoderAttentionModule5.0.3.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	2462638344608 -> 2462638952688
	2462638952688 [label=AccumulateGrad]
	2462638952640 -> 2462638952544
	2462638344688 [label="decoderAttentionModule5.0.3.bias
 (512)" fillcolor=lightblue]
	2462638344688 -> 2462638952640
	2462638952640 [label=AccumulateGrad]
	2462638952496 -> 2462638952448
	2462638344768 [label="decoderAttentionModule5.0.4.weight
 (512)" fillcolor=lightblue]
	2462638344768 -> 2462638952496
	2462638952496 [label=AccumulateGrad]
	2462638952160 -> 2462638952448
	2462638344848 [label="decoderAttentionModule5.0.4.bias
 (512)" fillcolor=lightblue]
	2462638344848 -> 2462638952160
	2462638952160 [label=AccumulateGrad]
	2462638532512 -> 2462638952256
	2462638952016 -> 2462638951872
	2462638343248 [label="decoderAttentionConv4.0.0.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	2462638343248 -> 2462638952016
	2462638952016 [label=AccumulateGrad]
	2462638951968 -> 2462638951872
	2462638343328 [label="decoderAttentionConv4.0.0.bias
 (256)" fillcolor=lightblue]
	2462638343328 -> 2462638951968
	2462638951968 [label=AccumulateGrad]
	2462638951824 -> 2462638951776
	2462638343408 [label="decoderAttentionConv4.0.1.weight
 (256)" fillcolor=lightblue]
	2462638343408 -> 2462638951824
	2462638951824 [label=AccumulateGrad]
	2462638951584 -> 2462638951776
	2462638343488 [label="decoderAttentionConv4.0.1.bias
 (256)" fillcolor=lightblue]
	2462638343488 -> 2462638951584
	2462638951584 [label=AccumulateGrad]
	2462638951440 -> 2462638951296
	2462638096064 [label="decoderAttentionModule4.0.0.weight
 (256, 768, 1, 1)" fillcolor=lightblue]
	2462638096064 -> 2462638951440
	2462638951440 [label=AccumulateGrad]
	2462638951392 -> 2462638951296
	2462638096144 [label="decoderAttentionModule4.0.0.bias
 (256)" fillcolor=lightblue]
	2462638096144 -> 2462638951392
	2462638951392 [label=AccumulateGrad]
	2462638951248 -> 2462638951200
	2462638096224 [label="decoderAttentionModule4.0.1.weight
 (256)" fillcolor=lightblue]
	2462638096224 -> 2462638951248
	2462638951248 [label=AccumulateGrad]
	2462638951104 -> 2462638951200
	2462638096304 [label="decoderAttentionModule4.0.1.bias
 (256)" fillcolor=lightblue]
	2462638096304 -> 2462638951104
	2462638951104 [label=AccumulateGrad]
	2462638950960 -> 2462638950816
	2462638342608 [label="decoderAttentionModule4.0.3.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2462638342608 -> 2462638950960
	2462638950960 [label=AccumulateGrad]
	2462638950912 -> 2462638950816
	2462638342688 [label="decoderAttentionModule4.0.3.bias
 (256)" fillcolor=lightblue]
	2462638342688 -> 2462638950912
	2462638950912 [label=AccumulateGrad]
	2462638950768 -> 2462638950720
	2462638342768 [label="decoderAttentionModule4.0.4.weight
 (256)" fillcolor=lightblue]
	2462638342768 -> 2462638950768
	2462638950768 [label=AccumulateGrad]
	2462638950432 -> 2462638950720
	2462638342848 [label="decoderAttentionModule4.0.4.bias
 (256)" fillcolor=lightblue]
	2462638342848 -> 2462638950432
	2462638950432 [label=AccumulateGrad]
	2462638530976 -> 2462638950528
	2462638950192 -> 2462638949712
	2462638095424 [label="decoderAttentionConv3.0.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	2462638095424 -> 2462638950192
	2462638950192 [label=AccumulateGrad]
	2462638950000 -> 2462638949712
	2462638095504 [label="decoderAttentionConv3.0.0.bias
 (128)" fillcolor=lightblue]
	2462638095504 -> 2462638950000
	2462638950000 [label=AccumulateGrad]
	2462638949520 -> 2462638949424
	2462638095584 [label="decoderAttentionConv3.0.1.weight
 (128)" fillcolor=lightblue]
	2462638095584 -> 2462638949520
	2462638949520 [label=AccumulateGrad]
	2462638948848 -> 2462638949424
	2462638095664 [label="decoderAttentionConv3.0.1.bias
 (128)" fillcolor=lightblue]
	2462638095664 -> 2462638948848
	2462638948848 [label=AccumulateGrad]
	2462638538608 -> 2462638538224
	2462638093984 [label="decoderAttentionModule3.0.0.weight
 (128, 384, 1, 1)" fillcolor=lightblue]
	2462638093984 -> 2462638538608
	2462638538608 [label=AccumulateGrad]
	2462638948464 -> 2462638538224
	2462638094064 [label="decoderAttentionModule3.0.0.bias
 (128)" fillcolor=lightblue]
	2462638094064 -> 2462638948464
	2462638948464 [label=AccumulateGrad]
	2462638538128 -> 2462638537936
	2462638094144 [label="decoderAttentionModule3.0.1.weight
 (128)" fillcolor=lightblue]
	2462638094144 -> 2462638538128
	2462638538128 [label=AccumulateGrad]
	2462638537648 -> 2462638537936
	2462638094224 [label="decoderAttentionModule3.0.1.bias
 (128)" fillcolor=lightblue]
	2462638094224 -> 2462638537648
	2462638537648 [label=AccumulateGrad]
	2462638537168 -> 2462638536688
	2462638094704 [label="decoderAttentionModule3.0.3.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2462638094704 -> 2462638537168
	2462638537168 [label=AccumulateGrad]
	2462638537072 -> 2462638536688
	2462638094784 [label="decoderAttentionModule3.0.3.bias
 (128)" fillcolor=lightblue]
	2462638094784 -> 2462638537072
	2462638537072 [label=AccumulateGrad]
	2462638536592 -> 2462638536400
	2462638094864 [label="decoderAttentionModule3.0.4.weight
 (128)" fillcolor=lightblue]
	2462638094864 -> 2462638536592
	2462638536592 [label=AccumulateGrad]
	2462638535536 -> 2462638536400
	2462638094944 [label="decoderAttentionModule3.0.4.bias
 (128)" fillcolor=lightblue]
	2462638094944 -> 2462638535536
	2462638535536 [label=AccumulateGrad]
	2462638529440 -> 2462638535728
	2462638535056 -> 2462638534576
	2462638093264 [label="decoderAttentionConv2.0.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	2462638093264 -> 2462638535056
	2462638535056 [label=AccumulateGrad]
	2462638534864 -> 2462638534576
	2462638093344 [label="decoderAttentionConv2.0.0.bias
 (64)" fillcolor=lightblue]
	2462638093344 -> 2462638534864
	2462638534864 [label=AccumulateGrad]
	2462638534384 -> 2462638534192
	2462638093424 [label="decoderAttentionConv2.0.1.weight
 (64)" fillcolor=lightblue]
	2462638093424 -> 2462638534384
	2462638534384 [label=AccumulateGrad]
	2462638533712 -> 2462638534192
	2462638093504 [label="decoderAttentionConv2.0.1.bias
 (64)" fillcolor=lightblue]
	2462638093504 -> 2462638533712
	2462638533712 [label=AccumulateGrad]
	2462638533232 -> 2462638532464
	2462638092064 [label="decoderAttentionModule2.0.0.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	2462638092064 -> 2462638533232
	2462638533232 [label=AccumulateGrad]
	2462638533040 -> 2462638532464
	2462638092144 [label="decoderAttentionModule2.0.0.bias
 (64)" fillcolor=lightblue]
	2462638092144 -> 2462638533040
	2462638533040 [label=AccumulateGrad]
	2462638532944 -> 2462638532368
	2462638092224 [label="decoderAttentionModule2.0.1.weight
 (64)" fillcolor=lightblue]
	2462638092224 -> 2462638532944
	2462638532944 [label=AccumulateGrad]
	2462638531984 -> 2462638532368
	2462638092304 [label="decoderAttentionModule2.0.1.bias
 (64)" fillcolor=lightblue]
	2462638092304 -> 2462638531984
	2462638531984 [label=AccumulateGrad]
	2462638531504 -> 2462638531024
	2462638092624 [label="decoderAttentionModule2.0.3.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2462638092624 -> 2462638531504
	2462638531504 [label=AccumulateGrad]
	2462638531408 -> 2462638531024
	2462638092704 [label="decoderAttentionModule2.0.3.bias
 (64)" fillcolor=lightblue]
	2462638092704 -> 2462638531408
	2462638531408 [label=AccumulateGrad]
	2462638530928 -> 2462638530832
	2462638092784 [label="decoderAttentionModule2.0.4.weight
 (64)" fillcolor=lightblue]
	2462638092784 -> 2462638530928
	2462638530928 [label=AccumulateGrad]
	2462638529872 -> 2462638530832
	2462638092864 [label="decoderAttentionModule2.0.4.bias
 (64)" fillcolor=lightblue]
	2462638092864 -> 2462638529872
	2462638529872 [label=AccumulateGrad]
	2462638528384 -> 2462638530160
	2462638529392 -> 2462638528912
	2462638091344 [label="decoderAttentionConv1.0.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2462638091344 -> 2462638529392
	2462638529392 [label=AccumulateGrad]
	2462638529296 -> 2462638528912
	2462638091424 [label="decoderAttentionConv1.0.0.bias
 (64)" fillcolor=lightblue]
	2462638091424 -> 2462638529296
	2462638529296 [label=AccumulateGrad]
	2462638528816 -> 2462638528624
	2462638091504 [label="decoderAttentionConv1.0.1.weight
 (64)" fillcolor=lightblue]
	2462638091504 -> 2462638528816
	2462638528816 [label=AccumulateGrad]
	2462638528336 -> 2462638528624
	2462638091584 [label="decoderAttentionConv1.0.1.bias
 (64)" fillcolor=lightblue]
	2462638091584 -> 2462638528336
	2462638528336 [label=AccumulateGrad]
	2462638528000 -> 2462638527856
	2462638089984 [label="decoderAttentionModule1.0.0.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	2462638089984 -> 2462638528000
	2462638528000 [label=AccumulateGrad]
	2462638527952 -> 2462638527856
	2462638090064 [label="decoderAttentionModule1.0.0.bias
 (64)" fillcolor=lightblue]
	2462638090064 -> 2462638527952
	2462638527952 [label=AccumulateGrad]
	2462638527808 -> 2462638527760
	2462638090144 [label="decoderAttentionModule1.0.1.weight
 (64)" fillcolor=lightblue]
	2462638090144 -> 2462638527808
	2462638527808 [label=AccumulateGrad]
	2462638527664 -> 2462638527760
	2462638090224 [label="decoderAttentionModule1.0.1.bias
 (64)" fillcolor=lightblue]
	2462638090224 -> 2462638527664
	2462638527664 [label=AccumulateGrad]
	2462638527520 -> 2462638527376
	2462638090624 [label="decoderAttentionModule1.0.3.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2462638090624 -> 2462638527520
	2462638527520 [label=AccumulateGrad]
	2462638527472 -> 2462638527376
	2462638090704 [label="decoderAttentionModule1.0.3.bias
 (64)" fillcolor=lightblue]
	2462638090704 -> 2462638527472
	2462638527472 [label=AccumulateGrad]
	2462638527328 -> 2462638527280
	2462638090784 [label="decoderAttentionModule1.0.4.weight
 (64)" fillcolor=lightblue]
	2462638090784 -> 2462638527328
	2462638527328 [label=AccumulateGrad]
	2462638527184 -> 2462638527280
	2462638090864 [label="decoderAttentionModule1.0.4.bias
 (64)" fillcolor=lightblue]
	2462638090864 -> 2462638527184
	2462638527184 [label=AccumulateGrad]
	2462638527088 -> 2462638526944
	2462638527088 [label=ReluBackward0]
	2462638527616 -> 2462638527088
	2462638527616 [label=CudnnBatchNormBackward0]
	2462638527904 -> 2462638527616
	2462638527904 [label=ConvolutionBackward0]
	2462638529104 -> 2462638527904
	2462638529104 [label=ReluBackward0]
	2462638530640 -> 2462638529104
	2462638530640 [label=CudnnBatchNormBackward0]
	2462638531888 -> 2462638530640
	2462638531888 [label=ConvolutionBackward0]
	2462638528240 -> 2462638531888
	2462638533904 -> 2462638531888
	2462534210224 [label="decoderBlock1.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2462534210224 -> 2462638533904
	2462638533904 [label=AccumulateGrad]
	2462638533520 -> 2462638531888
	2462534210064 [label="decoderBlock1.0.bias
 (64)" fillcolor=lightblue]
	2462534210064 -> 2462638533520
	2462638533520 [label=AccumulateGrad]
	2462638531216 -> 2462638530640
	2462534209984 [label="decoderBlock1.1.weight
 (64)" fillcolor=lightblue]
	2462534209984 -> 2462638531216
	2462638531216 [label=AccumulateGrad]
	2462638529968 -> 2462638530640
	2462534209904 [label="decoderBlock1.1.bias
 (64)" fillcolor=lightblue]
	2462534209904 -> 2462638529968
	2462638529968 [label=AccumulateGrad]
	2462638528432 -> 2462638527904
	2462534209104 [label="decoderBlock1.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2462534209104 -> 2462638528432
	2462638528432 [label=AccumulateGrad]
	2462638528144 -> 2462638527904
	2462534208944 [label="decoderBlock1.3.bias
 (64)" fillcolor=lightblue]
	2462534208944 -> 2462638528144
	2462638528144 [label=AccumulateGrad]
	2462638527712 -> 2462638527616
	2462534208864 [label="decoderBlock1.4.weight
 (64)" fillcolor=lightblue]
	2462534208864 -> 2462638527712
	2462638527712 [label=AccumulateGrad]
	2462638527232 -> 2462638527616
	2462534208784 [label="decoderBlock1.4.bias
 (64)" fillcolor=lightblue]
	2462534208784 -> 2462638527232
	2462638527232 [label=AccumulateGrad]
	2462638526896 -> 2462638526800
	2462638595728 [label="prediction_task1.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2462638595728 -> 2462638526896
	2462638526896 [label=AccumulateGrad]
	2462638526704 -> 2462638526800
	2462638595808 [label="prediction_task1.0.bias
 (64)" fillcolor=lightblue]
	2462638595808 -> 2462638526704
	2462638526704 [label=AccumulateGrad]
	2462638526752 -> 2462533885840
	2462638595968 [label="prediction_task1.1.weight
 (13, 64, 1, 1)" fillcolor=lightblue]
	2462638595968 -> 2462638526752
	2462638526752 [label=AccumulateGrad]
	2462638526656 -> 2462533885840
	2462638596048 [label="prediction_task1.1.bias
 (13)" fillcolor=lightblue]
	2462638596048 -> 2462638526656
	2462638526656 [label=AccumulateGrad]
	2462533884112 -> 2462638835168
}
